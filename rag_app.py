import logging
import dotenv
import os
import numpy as np
import re
import chromadb
from sentence_transformers import SentenceTransformer

import streamlit as st
import tempfile

from utils.loader import fetch_info
from utils.chunk import chunk_document, generate_chunk_context
from utils.vector_embed import VectorEmbedder
from utils.bm25 import create_bm25_index

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import warnings
warnings.filterwarnings("ignore")


# Suppress logs from a specific library (replace 'library_name' with the actual logger name)
logging.getLogger('chromadb').setLevel(logging.WARNING)


dotenv.load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

embed_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

llm = ChatOpenAI(
    model="gpt-4o",
    temperature=1.0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=OPENAI_API_KEY,
    streaming=True,
)

client = chromadb.PersistentClient("chroma.db")

collection = client.get_or_create_collection(name="doc_collection")

# embed_model = SentenceTransformer(
#     'sentence-transformers/all-MiniLM-L6-v2', device='mps')


# Function to check if the db already exists
def check_exixting_db(collection):
    if collection.get():
        return True
    return False

# Function to generate and stream the response from LLM


def llm_response(query, top_chunks, response_placeholder):
    # Combine the top chunks into a single prompt
    combined_text = "\n\n".join(top_chunks)

    # Generate LLM response based on the retrieved chunks
    messages = [
        {
            "role": "user",
            "content": f"""
                    Context:\n{combined_text}\n\nQuery: {query}\n\nInstructions:
                    If the query is directly related to the context provide, provide an accurate and relevant answer based on the context.
                    If the query is a general or unrelated question (e.g., greetings, or questions unrelated to the document),make up a response based on your data and respond in a generalized manner without referring to the document context.
                    For answers to question you do not know refer to your data, or search the web for the answer.
                    Examples:

                    1. Query: "Hi, how are you?"
                    - Response: "Hello! Iâ€™m doing great, thank you. How can I assist you today?"

                    2. Query: "What is the capital of France?"
                    - Response: "The capital of France is Paris."

                    4. Query: "Give an example specific to your document"
                    - Response: "Give the probable answer based on the context of the document."

                    For queries unrelated to the document, answer in a general manner without referencing any document content.
                    """
        },
    ]

    full_response = ""
    for chunk in llm.stream(messages):
        full_response += chunk.content
        response_placeholder.markdown(full_response + "â–Œ")
    response_placeholder.markdown(full_response)
    return full_response


def query_and_generate(uploaded_files, query, response_placeholder):
    embedder = VectorEmbedder(embed_model, collection)
    all_info = []
    for file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp:
            temp.write(file.read())
            temp_path = temp.name

        text = fetch_info(temp_path)[0].page_content
        all_info.append(text)

    # Creating chunks of all the documents and storing each in a list
    chunks = chunk_document(all_info)

    # List to store chunks with context generated by LLM
    contextualized_chunks = []

    # Checking for existing db, if not present, generate embeddings
    for chunk in chunks:
        context = generate_chunk_context(" ".join(all_info), chunk, llm)
        contextualized_chunks.append(context)

    if not check_exixting_db(collection):
        print("Generating embeddings for the first time")
        embedder.generate_embeddings(contextualized_chunks)

    # Creating bm25 index
    bm25 = create_bm25_index(contextualized_chunks)

    tokenzied_query = re.findall(r'\w+', query.lower())

    # Querying the bm25 index and getting top 10 results
    bm25_scores = bm25.get_scores(tokenzied_query)
    bm25_scores = sorted(bm25_scores, reverse=True)[:10]

    # Querying the vector db and getting top 10 results
    query_embedding = embedder.embedding_model.embed_query(query)

    db_results = embedder.collection.query(
        query_embeddings=query_embedding,
        n_results=10
    )

    # Extracting the scores and ids from the db results
    db_scores = []
    db_ids = []

    db_scores = db_results['distances'][0]
    db_ids = db_results['ids'][0]

    # Normalizing the scores and combining them
    bm25_scores_normalized = (
        bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))

    db_scores_inverted = [1 - score for score in db_scores]
    db_scores_normalized = (
        db_scores_inverted - np.min(db_scores_inverted)) / (np.max(db_scores_inverted) - np.min(db_scores_inverted))

    combined_scores = np.zeros(len(contextualized_chunks))

    all_scores = zip(db_ids, db_scores_normalized)

    all_scores = list(all_scores)

    for i, (chunk_id, score) in enumerate(all_scores):
        combined_scores[int(chunk_id)] = 0.5 * \
            bm25_scores_normalized[i] + 0.5 * score

    # Getting the top 10 chunks based on the combined scores
    top_n_indices = np.argsort(combined_scores)[::-1][:10]

    top_chunks = [contextualized_chunks[i] for i in top_n_indices]

    response = llm_response(query, top_chunks, response_placeholder)

    return response


def main():
    st.title("AI Assistant ðŸ¤–")

    uploaded_files = st.file_uploader(
        "Upload your documents", accept_multiple_files=True, type=['pdf'])
    query = st.text_input("Enter your query")

    if st.button("Generate Response"):
        if uploaded_files and query:
            response_placeholder = st.empty()
            response = query_and_generate(
                uploaded_files, query, response_placeholder)
        else:
            st.write("Please upload files and enter a query")


if __name__ == "__main__":
    main()
